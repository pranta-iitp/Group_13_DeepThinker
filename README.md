# Group_13_DeepThinker

# ğŸ§  Mini RAG-Powered Assistant

## ğŸ“Œ Overview

This project implements a **Mini RAG (Retrieval-Augmented Generation) Assistant** capable of answering user queries based on a **custom document corpus**.
The assistant uses **embeddings**, a **vector database**, and an **LLM** to retrieve relevant context and generate accurate, grounded responses.

This project demonstrates understanding of:

* GenAI fundamentals
* RAG pipeline
* Embedding generation
* Vector search
* Cloud deployment (Azure preferred)
* Node.js integration
* GitHub-based workflow

---

## ğŸš€ Architecture

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Document Corpus     â”‚
          â”‚  (PDFs / Articles)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
            Preprocessing & Chunking
                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Embedding Model  â”‚
          â”‚ (OpenAI / HF etc.) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Vector Store        â”‚
          â”‚ (FAISS / Pinecone)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Similarity Search
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Node.js Query Layer â”‚
          â”‚ (API / Web UI / CLI)â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Context Passing
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚     LLM (Gen AI)   â”‚
          â”‚ ChatGPT / OpenSourceâ”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Final Answer     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ 1. Corpus Preparation

* Selected **3â€“5 documents** (PDFs, technical articles, sample internal docs).
* Performed:

  * Cleaning (removing headers, footers, symbols)
  * Chunking (using sliding window or fixed chunk size)
  * Converting text to JSON/Markdown chunks

---

## ğŸ” 2. Embeddings & Vector Store

### Embedding Model Used: google/flan-t5-base

### Vector Database

* **FAISS** (local, lightweight, fast)

---

## â“ 3. Query Handling Flow

### User Query (Node.js frontend)

* Simple UI or CLI where user types a question.

### Steps

1. Convert user query â†’ embedding
2. Perform **similarity search** in FAISS
3. Retrieve top-K relevant chunks
4. Pass context + user prompt â†’ LLM
5. LLM generates final grounded answer

### Tech Stack

* Node.js (API + frontend)
* Express.js (optional)
* HTML/JS frontend (optional minimal UI)

---

## â˜ï¸ 4. Cloud Deployment

* Deploy backend on **Azure App Service / Azure Container Apps**
* Store vector DB locally or on cloud storage
* Use **GitHub** for version control
* Optionally integrate **GitHub Copilot** for faster development

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/pranta-iitp/Group_13_DeepThinker.git
cd mini-rag-assistant
```

### 2ï¸âƒ£ Install Dependencies

```bash
npm install
```

### 3ï¸âƒ£ Environment Variables

Create a `.env` file:

```
OPENAI_API_KEY=your_key_here
PORT=3000
```

### 4ï¸âƒ£ Build Embeddings

```bash
node scripts/embed.js
```

### 5ï¸âƒ£ Run the App

```bash
node server.js
```

### 6ï¸âƒ£ Access UI

```
http://localhost:3000
```

---

## ğŸ“¦ Folder Structure

```
ğŸ“ mini-rag-assistant
â”‚â”€â”€ ğŸ“ data/               # Raw documents
â”‚â”€â”€ ğŸ“ chunks/             # Preprocessed & chunked data
â”‚â”€â”€ ğŸ“ vectorstore/        # FAISS or Pinecone index
â”‚â”€â”€ ğŸ“ scripts/            # ETL scripts (chunking, embedding)
â”‚â”€â”€ server.js              # Node.js backend
â”‚â”€â”€ index.html             # Basic UI (optional)
â”‚â”€â”€ README.md
â”‚â”€â”€ .env
```

---

## ğŸ“˜ Learnings

### âœ” Understanding of RAG Pipeline

Learned how documents â†’ chunks â†’ embeddings â†’ vector search â†’ LLM response are connected.

### âœ” Working with Vector Databases

FAISS/Chroma helped understand:

* similarity search
* vector indexing
* retrieval performance

### âœ” Cloud Deployment Experience

Deployed on Azure to practice:

* environment variables
* scalability
* CI/CD using GitHub

### âœ” Node.js Integration

Learned building a minimal API to handle:

* queries
* LLM calls
* retrieval logic

---

## âš ï¸ Challenges & Solutions

### **1. Chunking Strategy Issues**

**Problem:** Poor chunking â†’ irrelevant search results
**Fix:** Used overlapping window technique for better context retention.

### **2. Embedding Cost / Speed**

**Fix:** Switched to compact models like MiniLM or `text-embedding-small`.

### **3. FAISS Index Not Loading Correctly**

**Fix:** Stored index + metadata separately and validated dimensions.

### **4. Cloud Deployment Errors**

**Fix:** Updated Node.js version & added correct environment variables in Azure portal.

---

## ğŸ“„ Future Improvements

* Add caching layer for faster responses
* Add authentication
* Add chat history memory
* Deploy vector store to Pinecone / Azure Cognitive Search
* Convert UI into full React app

---

## ğŸ™Œ Acknowledgements

* OpenAI / Azure OpenAI
* HuggingFace sentence-transformers
* FAISS / Pinecone
* Node.js community

---
