# Group_13_DeepThinker

# ğŸ§  Mini RAG-Powered Assistant

## ğŸ“Œ Overview

This project implements a **Mini RAG (Retrieval-Augmented Generation) Assistant** capable of answering user queries based on a **custom knowledge base of Indian folktales**.
The assistant uses **embeddings**, a **vector database**, and an **LLM** to retrieve relevant context and generate accurate, grounded responses. This approach significantly reduces hallucination and improves factual and contextual accuracy.

Objective

* Design and implement an end-to-end RAG pipeline
* Use Sentence Transformers for semantic embeddings
* Use FAISS as a vector database for similarity search
* Use FLAN-T5 for grounded answer generation
* Demonstrate understanding of GenAI system architecture
* Cloud Deployment

---

## ğŸš€ Architecture

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Document Corpus     â”‚
          â”‚  (PDFs / Articles)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
            Preprocessing & Chunking
                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Embedding Model  â”‚
          â”‚ (OpenAI / HF etc.) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Vector Store        â”‚
          â”‚ (FAISS / Pinecone)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Similarity Search
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Node.js Query Layer â”‚
          â”‚ (API / Web UI / CLI)â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Context Passing
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚     LLM (Gen AI)   â”‚
          â”‚ ChatGPT / OpenSourceâ”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Final Answer     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ 1. Corpus Preparation

* Selected **3â€“5 documents** (PDFs, technical articles, sample internal docs).
* Performed:

  * Cleaning (removing headers, footers, symbols)
  * Chunking (using sliding window or fixed chunk size)
  * Converting text to JSON/Markdown chunks

---

## ğŸ” 2. Embeddings & Vector Store

### Embedding Model Used: 

* **google/flan-t5-base**

### Vector Database

* **FAISS** (local, lightweight, fast)

---

## â“ 3. Query Handling Flow

### User Query (Node.js frontend)

* Simple UI or CLI where user types a question.

### Steps

1. Convert user query â†’ embedding
2. Perform **similarity search** in FAISS
3. Retrieve top-K relevant chunks
4. Pass context + user prompt â†’ LLM
5. LLM generates final grounded answer

### Tech Stack

* FLAN-T5 (LLM)

Why used:

Instruction-tuned encoderâ€“decoder model

Performs well when provided with external context

Lower computational cost compared to large decoder-only LLMs

Suitable for question answering and text generation tasks

Role in the system:

Generates final answers using retrieved folktale context

Ensures responses remain grounded and relevant
* Sentence Transformers

Model: all-MiniLM-L6-v2

Why used:

Produces high-quality semantic embeddings

Captures meaning beyond keyword matching

Fast and efficient on CPU

Widely adopted for semantic search applications

Role in the system:

Converts folktale text chunks and user queries into vector embeddings

Enables semantic similarity search
* FAISS (Vector Database)

Why used:

Optimized for fast similarity search over dense vectors

Lightweight and open-source

Works efficiently without cloud dependency

Scales well for medium-sized document collections

Role in the system:

Stores document embeddings

Retrieves top-K relevant folktale chunks for a given query

---

## â˜ï¸ 4. Cloud Deployment

* Deploy backend on **Azure App Service / Azure Container Apps**
* Store vector DB locally or on cloud storage
* Use **GitHub** for version control
* Optionally integrate **GitHub Copilot** for faster development

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/pranta-iitp/Group_13_DeepThinker.git
cd mini-rag-assistant
```

### 2ï¸âƒ£ Install Dependencies

```bash
npm install
```

### 3ï¸âƒ£ Environment Variables

Create a `.env` file:

```
OPENAI_API_KEY=your_key_here
PORT=3000
```

### 4ï¸âƒ£ Build Embeddings

```bash
node scripts/embed.js
```

### 5ï¸âƒ£ Run the App

```bash
node server.js
```

### 6ï¸âƒ£ Access UI

```
http://localhost:3000
```

---

## ğŸ“¦ Folder Structure

```
ğŸ“ mini-rag-assistant
â”‚â”€â”€ ğŸ“ data/               # Raw documents
â”‚â”€â”€ ğŸ“ chunks/             # Preprocessed & chunked data
â”‚â”€â”€ ğŸ“ vectorstore/        # FAISS or Pinecone index
â”‚â”€â”€ ğŸ“ scripts/            # ETL scripts (chunking, embedding)
â”‚â”€â”€ server.js              # Node.js backend
â”‚â”€â”€ index.html             # Basic UI (optional)
â”‚â”€â”€ README.md
â”‚â”€â”€ .env
```

---

## ğŸ“˜ Learnings

### âœ” Understanding of RAG Pipeline

Learned how documents â†’ chunks â†’ embeddings â†’ vector search â†’ LLM response are connected.

### âœ” Working with Vector Databases

FAISS/Chroma helped understand:

* similarity search
* vector indexing
* retrieval performance

### âœ” Cloud Deployment Experience

Deployed on Azure to practice:

* environment variables
* scalability
* CI/CD using GitHub

### âœ” Node.js Integration

Learned building a minimal API to handle:

* queries
* LLM calls
* retrieval logic

---

## âš ï¸ Challenges & Solutions

### **1. Chunking Strategy Issues**

**Problem:** Poor chunking â†’ irrelevant search results
**Fix:** Used overlapping window technique for better context retention.
