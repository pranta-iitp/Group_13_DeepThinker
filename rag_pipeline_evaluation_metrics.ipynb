{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJcJGEBDXMsf"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers faiss-cpu transformers torch gradio pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "3OiXc-MYYMpI",
        "outputId": "8018598a-d83f-409b-90cf-cb37f6187e7c"
      },
      "outputs": [],
      "source": [
        "# CELL 2\n",
        "# User decides how many PDFs to upload\n",
        "# Each PDF must contain exactly 5 stories with titles\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"INSTRUCTIONS FOR DATASET UPLOAD\")\n",
        "print(\"---------------------------------\")\n",
        "print(\"• Each PDF MUST contain exactly 5 stories\")\n",
        "print(\"• Each story MUST have a clear title\")\n",
        "print(\"• Story format should be: Story 1, Story 2, ..., Story 5\")\n",
        "print(\"---------------------------------\\n\")\n",
        "\n",
        "# Ask user how many PDFs they want to upload\n",
        "num_pdfs = int(input(\"How many PDF files do you want to upload? \"))\n",
        "\n",
        "pdf_files = []\n",
        "\n",
        "# Loop to upload PDFs one by one\n",
        "for i in range(num_pdfs):\n",
        "    print(f\"\\n Upload PDF {i+1} of {num_pdfs}\")\n",
        "    uploaded = files.upload()   # opens file chooser\n",
        "    for filename in uploaded.keys():\n",
        "        pdf_files.append(filename)\n",
        "\n",
        "print(\"\\n Upload complete!\")\n",
        "print(\"Uploaded PDF files:\")\n",
        "for pdf in pdf_files:\n",
        "    print(\"-\", pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhjiMOBXXPCP",
        "outputId": "56e4e269-6004-4baf-9aff-b05ed03a2183"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "\n",
        "def extract_stories_from_pdf(pdf_path):\n",
        "    # PDF se text nikalna\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "\n",
        "    # Text cleaning: Newlines hatana taaki sentence na toote\n",
        "    # \"Story 1\" dhundhne se pehle formatting fix karte hain\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # \"Story X\" pattern dhundhna\n",
        "    matches = list(re.finditer(r'Story\\s+\\d+', text))\n",
        "\n",
        "    stories = []\n",
        "    for i in range(len(matches)):\n",
        "        start = matches[i].start()\n",
        "        # Agli story kahan shuru hoti hai? Wahan tak text lo\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
        "\n",
        "        story_block = text[start:end].strip()\n",
        "\n",
        "        # Title nikalna: \"Story X\" ke baad pehla sentence title hota hai\n",
        "        # Hum pehle full stop (.) tak ka text title maante hain\n",
        "        first_dot = story_block.find('.')\n",
        "        if first_dot != -1:\n",
        "            title = story_block[:first_dot+1].strip()\n",
        "        else:\n",
        "            title = story_block[:50].strip() # Fallback\n",
        "\n",
        "        stories.append({\n",
        "            \"title\": title,\n",
        "            \"content\": story_block,\n",
        "            \"source\": pdf_path\n",
        "        })\n",
        "\n",
        "    return stories\n",
        "\n",
        "# Saari stories load karte hain\n",
        "all_stories = []\n",
        "for pdf in pdf_files:\n",
        "    stories = extract_stories_from_pdf(pdf)\n",
        "    all_stories.extend(stories)\n",
        "\n",
        "print(f\" Total stories extracted: {len(all_stories)}\")\n",
        "# Check karne ke liye pehli story print karte hain\n",
        "if all_stories:\n",
        "    print(\"\\nPreview Story 1 Title:\", all_stories[0]['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMZ1hTNYXPFD",
        "outputId": "f85d27fe-f12e-4d52-8bc7-7f3b667ce4d2"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# 1. Documents prepare karna\n",
        "documents = [s['content'] for s in all_stories]\n",
        "\n",
        "# 2. Model load karna\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 3. Embeddings banana\n",
        "embeddings = embedder.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "# 4. FAISS Index banana (Fast search ke liye)\n",
        "d = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"Embeddings & Index created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owN3QscJXPHn",
        "outputId": "acf41678-5c13-4a27-df2e-b55a06ddc420"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "print(\"FLAN-T5 Model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoHNKZ02XPKM"
      },
      "outputs": [],
      "source": [
        "def ask_akbar_birbal(question):\n",
        "    # 1. Sawal ko embed karo\n",
        "    q_emb = embedder.encode([question], convert_to_numpy=True)\n",
        "\n",
        "    # 2. Sirf TOP 1 matching story dhundo (taaki model confuse na ho)\n",
        "    k = 1\n",
        "    distances, indices = index.search(q_emb, k)\n",
        "\n",
        "    # Best matching story ka text\n",
        "    best_story_idx = indices[0][0]\n",
        "    retrieved_story = documents[best_story_idx]\n",
        "\n",
        "    print(f\"Reading relevant story... (Story Index: {best_story_idx})\")\n",
        "\n",
        "    # 3. Prompt banana (Strict instructions ke saath)\n",
        "    prompt = f\"\"\"\n",
        "    Read the story below and answer the question.\n",
        "\n",
        "    Story:\n",
        "    {retrieved_story}\n",
        "\n",
        "    Instructions:\n",
        "    1. Explain what happens in the story related to the question.\n",
        "    2. Provide the Moral of the story if mentioned or imply it.\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # 4. Answer generate karna\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=150,\n",
        "        min_length=20,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn1K5XXqXPPE",
        "outputId": "b5f54b4c-2397-45bc-c95d-67173164c495"
      },
      "outputs": [],
      "source": [
        "user_question = input(\" Ask a question (e.g., 'Ask a question about the stories?'): \")\n",
        "\n",
        "print(\"\\n\" + \"-\"*30)\n",
        "final_answer = ask_akbar_birbal(user_question)\n",
        "\n",
        "print(\"Birbal's Answer:\\n\")\n",
        "print(final_answer)\n",
        "print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk1lATTViV3m"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "9tVUCvZNhuNR",
        "outputId": "cc73da7e-cdd1-4043-d074-b29f5dd79c41"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# FINAL EVALUATION CELL (WORKS WITH YOUR RAG SYSTEM)\n",
        "# =====================================================\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload an Excel file with a 'Question' column\")\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Load Excel\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "if \"query\" not in df.columns:\n",
        "    raise ValueError(\"Excel must contain a column named 'Question'\")\n",
        "\n",
        "# Function that uses your RAG QA system\n",
        "def evaluate_question(q):\n",
        "    try:\n",
        "        return ask_akbar_birbal(q)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "print(\"Generating answers using your RAG pipeline...\")\n",
        "print(df[\"query\"])\n",
        "df[\"generated_answer\"] = df[\"query\"].apply(evaluate_question)\n",
        "\n",
        "# Save result\n",
        "output_file = \"evaluation_results.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Evaluation complete!\")\n",
        "print(\"Downloading:\", output_file)\n",
        "files.download(output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKRG8jsKqBRm",
        "outputId": "8d190ca7-055d-4488-8771-ca3c4ad9989d"
      },
      "outputs": [],
      "source": [
        "!pip install bert-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "8M_26n_8jg_u",
        "outputId": "f2fe841f-3b79-4de2-dcad-8c33db91678e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from bert_score import score\n",
        "from google.colab import files\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Upload Excel file\n",
        "# ---------------------------\n",
        "print(\"Please upload your Excel file (with generated_answer & ground_truth columns)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if len(uploaded) == 0:\n",
        "    raise ValueError(\"No file uploaded!\")\n",
        "\n",
        "input_file = list(uploaded.keys())[0]\n",
        "print(\"Uploaded file:\", input_file)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Read Excel file\n",
        "# ---------------------------\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "required_cols = [\"generated_answer\", \"ground_truth\"]\n",
        "for col in required_cols:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Missing required column: {col}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Prepare text for BERTScore\n",
        "# ---------------------------\n",
        "generated = df[\"generated_answer\"].astype(str).tolist()\n",
        "ground_truth = df[\"ground_truth\"].astype(str).tolist()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"⚙ Using device:\", device)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Compute BERTScore\n",
        "# ---------------------------\n",
        "print(\"Computing BERTScores...\")\n",
        "\n",
        "precision, recall, f1 = score(\n",
        "    generated,\n",
        "    ground_truth,\n",
        "    model_type=\"bert-base-uncased\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Add new columns to SAME file\n",
        "# ---------------------------\n",
        "df[\"bertscore_precision\"] = precision.cpu().numpy()\n",
        "df[\"bertscore_recall\"] = recall.cpu().numpy()\n",
        "df[\"bertscore_f1\"] = f1.cpu().numpy()\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Save updated Excel\n",
        "# ---------------------------\n",
        "output_file = \"bert_scored_output.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"BERTScore calculation completed.\")\n",
        "print(\"Downloading updated file:\", output_file)\n",
        "\n",
        "# Download\n",
        "files.download(output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIHJx8vAp3Sc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
